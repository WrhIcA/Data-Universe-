"""
Climate Tipping Point Early Warning System - Interactive Data Lab
Real-time monitoring, detection, and visualization of climate tipping point signals
with advanced machine learning and statistical analysis.

Integrates with Climate Tipping Point Algorithm for comprehensive Earth system monitoring.
Supports NASA AWG and Copernicus Program data integration.

Author: [Suraj Bahadur Silwal]
Orcid: https://orcid.org/0009-0002-7602-188X
Version: 1.0.0
Date: January 2026
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib.gridspec import GridSpec
import pandas as pd
from scipy import signal, stats, ndimage
from scipy.fft import fft, fftfreq
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from typing import List, Tuple, Dict, Optional, Union
import warnings
import datetime
import json
warnings.filterwarnings('ignore')

# Import from climate tipping point module
try:
    from climate_tipping_point import (
        ClimateIndicator, ClimateTippingPointModel, 
        TippingPointDetector, IntegratedClimateTippingFramework
    )
except ImportError:
    print("Warning: climate_tipping_point module not found. Some features may be limited.")


# ============================================================================
# SECTION 1: REAL-TIME DATA STREAM SIMULATOR
# ============================================================================

class ClimateDataStreamSimulator:
    """
    Simulates real-time climate data streams for testing and demonstration.
    In production, this would connect to actual satellite/sensor networks.
    """
    
    def __init__(self, indicators: List[str], baseline_values: Dict[str, float],
                 noise_levels: Dict[str, float]):
        """
        Initialize data stream simulator.
        
        Parameters:
        -----------
        indicators : list
            List of climate indicators to simulate
        baseline_values : dict
            Baseline value for each indicator
        noise_levels : dict
            Noise amplitude for each indicator
        """
        self.indicators = indicators
        self.baseline_values = baseline_values
        self.noise_levels = noise_levels
        self.current_time = 0
        self.data_buffer = {ind: [] for ind in indicators}
        self.time_buffer = []
        
        # Simulate approaching tipping point for some indicators
        self.trend_rates = {}
        for ind in indicators:
            # Random trend toward threshold
            self.trend_rates[ind] = np.random.uniform(-0.001, 0.002)
    
    def generate_sample(self) -> Dict[str, float]:
        """
        Generate one time step of data.
        
        Returns:
        --------
        sample : dict
            Current values for all indicators
        """
        sample = {}
        
        for ind in self.indicators:
            # Base value with trend
            base = self.baseline_values[ind] + self.trend_rates[ind] * self.current_time
            
            # Add autocorrelated noise (simulates critical slowing down)
            if len(self.data_buffer[ind]) > 0:
                # Autocorrelation increases near tipping point
                autocorr_factor = 0.7 + 0.2 * (self.current_time / 1000)
                noise = autocorr_factor * np.random.randn() * self.noise_levels[ind]
                previous = self.data_buffer[ind][-1]
                value = 0.3 * previous + 0.7 * (base + noise)
            else:
                value = base + np.random.randn() * self.noise_levels[ind]
            
            sample[ind] = value
            self.data_buffer[ind].append(value)
        
        self.time_buffer.append(self.current_time)
        self.current_time += 1
        
        return sample
    
    def get_buffer(self, window: int = None) -> Dict[str, np.ndarray]:
        """
        Get buffered data.
        
        Parameters:
        -----------
        window : int
            Number of recent samples to return (None = all)
        
        Returns:
        --------
        data : dict
            Buffered data arrays
        """
        if window is None:
            return {ind: np.array(vals) for ind, vals in self.data_buffer.items()}
        else:
            return {ind: np.array(vals[-window:]) for ind, vals in self.data_buffer.items()}


# ============================================================================
# SECTION 2: EARLY WARNING SIGNAL ANALYZER
# ============================================================================

class EarlyWarningAnalyzer:
    """
    Advanced early warning signal detection and analysis.
    """
    
    def __init__(self, window_size: int = 100):
        """
        Initialize analyzer.
        
        Parameters:
        -----------
        window_size : int
            Size of rolling window for analysis
        """
        self.window_size = window_size
        self.alert_history = []
    
    def critical_slowing_down(self, data: np.ndarray) -> Dict:
        """
        Comprehensive critical slowing down (CSD) analysis.
        
        CSD indicators:
        - Increasing autocorrelation at lag-1
        - Increasing variance
        - Increasing recovery time from perturbations
        
        Parameters:
        -----------
        data : np.ndarray
            Time series data
            
        Returns:
        --------
        csd_metrics : dict
            Critical slowing down indicators
        """
        if len(data) < self.window_size:
            return {
                'warning_level': 0,
                'status': 'Insufficient data',
                'autocorr': 0,
                'variance': 0,
                'recovery_time': 0
            }
        
        # Detrend the data
        x = np.arange(len(data))
        trend_coeffs = np.polyfit(x, data, 1)
        trend = np.polyval(trend_coeffs, x)
        detrended = data - trend
        
        # Rolling window analysis
        n_windows = len(data) - self.window_size + 1
        autocorr_series = []
        variance_series = []
        recovery_series = []
        
        for i in range(n_windows):
            window = detrended[i:i+self.window_size]
            
            # Autocorrelation at lag-1
            if len(window) > 1:
                ac = np.corrcoef(window[:-1], window[1:])[0, 1]
                autocorr_series.append(ac if not np.isnan(ac) else 0)
            
            # Variance
            variance_series.append(np.var(window))
            
            # Recovery time (exponential decay fit)
            if np.std(window) > 0:
                # Find perturbations (values > 1 std)
                perturbations = np.where(np.abs(window) > np.std(window))[0]
                if len(perturbations) > 5:
                    # Measure average recovery time
                    recovery_times = []
                    for p in perturbations[:10]:  # Sample first 10
                        if p < len(window) - 10:
                            decay = np.abs(window[p:p+10])
                            # Fit exponential
                            try:
                                t = np.arange(len(decay))
                                popt, _ = np.polyfit(t, np.log(decay + 1e-10), 1, cov=True)
                                tau = -1 / popt[0] if popt[0] < 0 else 0
                                recovery_times.append(tau)
                            except:
                                pass
                    if recovery_times:
                        recovery_series.append(np.mean(recovery_times))
        
        # Trends in indicators (positive trends = warning)
        autocorr_trend = 0
        variance_trend = 0
        recovery_trend = 0
        
        if len(autocorr_series) > 10:
            t = np.arange(len(autocorr_series))
            autocorr_trend = np.polyfit(t, autocorr_series, 1)[0]
            variance_trend = np.polyfit(t, variance_series, 1)[0]
        
        if len(recovery_series) > 10:
            t = np.arange(len(recovery_series))
            recovery_trend = np.polyfit(t, recovery_series, 1)[0]
        
        # Calculate warning level (0-1)
        warning_level = 0
        
        # Autocorrelation increasing
        if autocorr_trend > 0:
            warning_level += min(autocorr_trend * 10, 0.33)
        
        # Variance increasing
        if variance_trend > 0:
            warning_level += min(variance_trend * 10, 0.33)
        
        # Recovery time increasing
        if recovery_trend > 0:
            warning_level += min(recovery_trend * 0.1, 0.34)
        
        warning_level = min(warning_level, 1.0)
        
        # Status
        if warning_level > 0.7:
            status = "CRITICAL WARNING"
        elif warning_level > 0.5:
            status = "HIGH WARNING"
        elif warning_level > 0.3:
            status = "MODERATE WARNING"
        else:
            status = "STABLE"
        
        return {
            'warning_level': warning_level,
            'status': status,
            'autocorr': autocorr_series[-1] if autocorr_series else 0,
            'autocorr_trend': autocorr_trend,
            'variance': variance_series[-1] if variance_series else 0,
            'variance_trend': variance_trend,
            'recovery_time': recovery_series[-1] if recovery_series else 0,
            'recovery_trend': recovery_trend,
            'autocorr_series': autocorr_series,
            'variance_series': variance_series
        }
    
    def spectral_reddening(self, data: np.ndarray) -> Dict:
        """
        Detect spectral reddening - shift of power to lower frequencies.
        
        Near tipping points, high-frequency fluctuations decrease while
        low-frequency fluctuations increase (spectrum becomes "redder").
        
        Parameters:
        -----------
        data : np.ndarray
            Time series data
            
        Returns:
        --------
        spectral_metrics : dict
            Spectral analysis results
        """
        if len(data) < 50:
            return {'reddening_index': 0, 'status': 'Insufficient data'}
        
        # Detrend
        x = np.arange(len(data))
        trend = np.polyfit(x, data, 1)
        detrended = data - np.polyval(trend, x)
        
        # Compute power spectrum
        freqs = fftfreq(len(detrended))
        power = np.abs(fft(detrended))**2
        
        # Only positive frequencies
        pos_mask = freqs > 0
        freqs_pos = freqs[pos_mask]
        power_pos = power[pos_mask]
        
        # Divide into low and high frequency bands
        median_freq = np.median(freqs_pos)
        low_freq_mask = freqs_pos < median_freq
        high_freq_mask = freqs_pos >= median_freq
        
        low_freq_power = np.mean(power_pos[low_freq_mask])
        high_freq_power = np.mean(power_pos[high_freq_mask])
        
        # Reddening index: ratio of low to high frequency power
        if high_freq_power > 0:
            reddening_index = low_freq_power / high_freq_power
        else:
            reddening_index = 0
        
        # Fit power law: P(f) ~ f^β
        # β < 0 for white noise, β → -1 for pink noise, β → -2 for brown noise
        try:
            log_freqs = np.log10(freqs_pos[freqs_pos > 0])
            log_power = np.log10(power_pos[freqs_pos > 0])
            beta = np.polyfit(log_freqs, log_power, 1)[0]
        except:
            beta = 0
        
        # Warning level
        warning = 0
        if reddening_index > 2.0:
            warning = min((reddening_index - 2.0) / 5.0, 1.0)
        
        status = "WARNING" if warning > 0.5 else "STABLE"
        
        return {
            'reddening_index': reddening_index,
            'power_law_exponent': beta,
            'low_freq_power': low_freq_power,
            'high_freq_power': high_freq_power,
            'warning_level': warning,
            'status': status,
            'freqs': freqs_pos,
            'power': power_pos
        }
    
    def flickering_detection(self, data: np.ndarray, threshold: float = None) -> Dict:
        """
        Detect flickering - rapid transitions between alternative states.
        
        Parameters:
        -----------
        data : np.ndarray
            Time series data
        threshold : float
            Threshold for state definition (None = use median)
            
        Returns:
        --------
        flickering_metrics : dict
            Flickering detection results
        """
        if len(data) < 20:
            return {'flickering_rate': 0, 'status': 'Insufficient data'}
        
        if threshold is None:
            threshold = np.median(data)
        
        # Identify state (above or below threshold)
        states = (data > threshold).astype(int)
        
        # Count transitions
        transitions = np.sum(np.abs(np.diff(states)))
        
        # Flickering rate (transitions per time unit)
        flickering_rate = transitions / len(data)
        
        # Time in each state
        time_above = np.sum(states) / len(states)
        time_below = 1 - time_above
        
        # State balance (near 0.5 indicates bistability)
        balance = 1 - abs(time_above - 0.5) * 2  # 1 = perfect balance, 0 = always in one state
        
        # Warning level
        warning = 0
        if flickering_rate > 0.1 and balance > 0.3:
            warning = min(flickering_rate * 5 * balance, 1.0)
        
        status = "FLICKERING DETECTED" if warning > 0.5 else "STABLE"
        
        return {
            'flickering_rate': flickering_rate,
            'transitions': transitions,
            'time_above_threshold': time_above,
            'state_balance': balance,
            'warning_level': warning,
            'status': status
        }
    
    def composite_warning_score(self, data: np.ndarray, 
                               threshold: float = None) -> Dict:
        """
        Calculate composite early warning score from all indicators.
        
        Parameters:
        -----------
        data : np.ndarray
            Time series data
        threshold : float
            Threshold for flickering detection
            
        Returns:
        --------
        composite : dict
            Composite warning analysis
        """
        # Run all analyses
        csd = self.critical_slowing_down(data)
        spectral = self.spectral_reddening(data)
        flickering = self.flickering_detection(data, threshold)
        
        # Weighted composite score
        weights = {
            'csd': 0.4,
            'spectral': 0.3,
            'flickering': 0.3
        }
        
        composite_score = (
            weights['csd'] * csd['warning_level'] +
            weights['spectral'] * spectral['warning_level'] +
            weights['flickering'] * flickering['warning_level']
        )
        
        # Overall status
        if composite_score > 0.7:
            overall_status = "CRITICAL - IMMEDIATE ACTION REQUIRED"
            color = 'red'
        elif composite_score > 0.5:
            overall_status = "HIGH RISK - CLOSE MONITORING"
            color = 'orange'
        elif composite_score > 0.3:
            overall_status = "MODERATE RISK - WATCH CAREFULLY"
            color = 'yellow'
        else:
            overall_status = "LOW RISK - STABLE"
            color = 'green'
        
        # Generate alert if needed
        if composite_score > 0.5:
            alert = {
                'timestamp': datetime.datetime.now(),
                'score': composite_score,
                'status': overall_status,
                'indicators': {
                    'csd': csd['status'],
                    'spectral': spectral['status'],
                    'flickering': flickering['status']
                }
            }
            self.alert_history.append(alert)
        
        return {
            'composite_score': composite_score,
            'overall_status': overall_status,
            'status_color': color,
            'csd': csd,
            'spectral': spectral,
            'flickering': flickering,
            'timestamp': datetime.datetime.now()
        }


# ============================================================================
# SECTION 3: INTERACTIVE DASHBOARD
# ============================================================================

class EarlyWarningDashboard:
    """
    Real-time interactive dashboard for climate tipping point monitoring.
    """
    
    def __init__(self, indicators: List[str], update_interval: int = 100):
        """
        Initialize dashboard.
        
        Parameters:
        -----------
        indicators : list
            List of climate indicators to monitor
        update_interval : int
            Update interval in milliseconds
        """
        self.indicators = indicators
        self.update_interval = update_interval
        
        # Initialize data simulator
        baseline_values = {
            'temperature': 1.2,
            'arctic_ice': 4.5,
            'amoc': 85.0,
            'amazon': 80.0,
            'permafrost': 15.0,
            'glaciers': 70.0
        }
        
        noise_levels = {
            'temperature': 0.05,
            'arctic_ice': 0.1,
            'amoc': 1.0,
            'amazon': 0.5,
            'permafrost': 0.3,
            'glaciers': 0.4
        }
        
        self.data_stream = ClimateDataStreamSimulator(
            indicators, baseline_values, noise_levels
        )
        
        # Initialize analyzers for each indicator
        self.analyzers = {ind: EarlyWarningAnalyzer(window_size=100) 
                         for ind in indicators}
        
        # Warning thresholds
        self.thresholds = {
            'temperature': 1.5,
            'arctic_ice': 3.0,
            'amoc': 70.0,
            'amazon': 60.0,
            'permafrost': 25.0,
            'glaciers': 50.0
        }
        
        # Setup figure
        self.setup_dashboard()
    
    def setup_dashboard(self):
        """Create dashboard layout."""
        self.fig = plt.figure(figsize=(18, 12))
        gs = GridSpec(4, 4, figure=self.fig, hspace=0.4, wspace=0.4)
        
        # Main time series plots (top 2 rows, left 3 columns)
        self.ts_axes = {}
        for i, ind in enumerate(self.indicators[:6]):
            row = i // 3
            col = i % 3
            ax = self.fig.add_subplot(gs[row, col])
            ax.set_title(ind.replace('_', ' ').title(), fontweight='bold')
            ax.set_xlabel('Time')
            ax.set_ylabel('Value')
            ax.grid(True, alpha=0.3)
            self.ts_axes[ind] = ax
        
        # Warning level panel (top right)
        self.warning_ax = self.fig.add_subplot(gs[0:2, 3])
        self.warning_ax.set_title('Early Warning Levels', fontweight='bold', fontsize=12)
        self.warning_ax.set_xlim(0, 1)
        self.warning_ax.set_ylim(-0.5, len(self.indicators) - 0.5)
        self.warning_ax.set_xlabel('Warning Level')
        self.warning_ax.grid(True, alpha=0.3, axis='x')
        
        # CSD indicators panel (bottom left)
        self.csd_ax = self.fig.add_subplot(gs[2, 0:2])
        self.csd_ax.set_title('Critical Slowing Down Indicators', fontweight='bold')
        self.csd_ax.set_xlabel('Time')
        self.csd_ax.set_ylabel('Autocorrelation')
        self.csd_ax.grid(True, alpha=0.3)
        
        # Spectral analysis panel (bottom middle)
        self.spectral_ax = self.fig.add_subplot(gs[2, 2:4])
        self.spectral_ax.set_title('Power Spectrum', fontweight='bold')
        self.spectral_ax.set_xlabel('Frequency')
        self.spectral_ax.set_ylabel('Power')
        self.spectral_ax.set_yscale('log')
        self.spectral_ax.grid(True, alpha=0.3)
        
        # Alert log panel (bottom right)
        self.alert_ax = self.fig.add_subplot(gs[3, :])
        self.alert_ax.set_title('Alert Log', fontweight='bold')
        self.alert_ax.axis('off')
        
        self.fig.suptitle('Climate Tipping Point Early Warning System - Real-Time Dashboard',
                         fontsize=16, fontweight='bold')
    
    def update(self, frame):
        """Update dashboard with new data."""
        # Generate new data
        sample = self.data_stream.generate_sample()
        
        # Get buffered data
        data_buffer = self.data_stream.get_buffer(window=500)
        time_buffer = np.array(self.data_stream.time_buffer[-500:])
        
        # Update time series plots
        for ind in self.indicators[:6]:
            if ind in self.ts_axes and ind in data_buffer:
                ax = self.ts_axes[ind]
                ax.clear()
                
                # Plot data
                ax.plot(time_buffer, data_buffer[ind], 'b-', linewidth=1, alpha=0.7)
                
                # Plot threshold
                if ind in self.thresholds:
                    ax.axhline(self.thresholds[ind], color='red', 
                              linestyle='--', linewidth=2, alpha=0.7, label='Threshold')
                
                # Recent trend
                if len(data_buffer[ind]) > 20:
                    recent_data = data_buffer[ind][-20:]
                    recent_time = time_buffer[-20:]
                    trend = np.polyfit(recent_time - recent_time[0], recent_data, 1)
                    trend_line = np.polyval(trend, recent_time - recent_time[0])
                    ax.plot(recent_time, trend_line, 'r--', linewidth=2, alpha=0.5, label='Trend')
                
                ax.set_title(ind.replace('_', ' ').title(), fontweight='bold')
                ax.set_xlabel('Time')
                ax.set_ylabel('Value')
                ax.grid(True, alpha=0.3)
                ax.legend(fontsize=8, loc='upper left')
        
        # Update warning levels
        self.warning_ax.clear()
        warning_levels = []
        warning_colors = []
        
        for i, ind in enumerate(self.indicators):
            if ind in data_buffer and len(data_buffer[ind]) > 100:
                # Analyze
                analysis = self.analyzers[ind].composite_warning_score(
                    data_buffer[ind],
                    threshold=self.thresholds.get(ind)
                )
                
                warning_levels.append(analysis['composite_score'])
                warning_colors.append(analysis['status_color'])
            else:
                warning_levels.append(0)
                warning_colors.append('gray')
        
        # Plot warning bars
        y_pos = np.arange(len(self.indicators))
        self.warning_ax.barh(y_pos, warning_levels, color=warning_colors, alpha=0.7)
        
        # Add threshold line
        self.warning_ax.axvline(0.5, color='orange', linestyle='--', linewidth=2, alpha=0.5)
        self.warning_ax.axvline(0.7, color='red', linestyle='--', linewidth=2, alpha=0.5)
        
        self.warning_ax.set_yticks(y_pos)
        self.warning_ax.set_yticklabels([ind.replace('_', '\n') for ind in self.indicators])
        self.warning_ax.set_xlim(0, 1)
        self.warning_ax.set_ylim(-0.5, len(self.indicators) - 0.5)
        self.warning_ax.set_xlabel('Warning Level')
        self.warning_ax.set_title('Early Warning Levels', fontweight='bold')
        self.warning_ax.grid(True, alpha=0.3, axis='x')
        
        # Update CSD indicators (use first indicator as example)
        if self.indicators and self.indicators[0] in data_buffer:
            ind = self.indicators[0]
            if len(data_buffer[ind]) > 100:
                csd = self.analyzers[ind].critical_slowing_down(data_buffer[ind])
                
                if 'autocorr_series' in csd and csd['autocorr_series']:
                    self.csd_ax.clear()
                    ac_time = time_buffer[-len(csd['autocorr_series']):]
                    self.csd_ax.plot(ac_time, csd['autocorr_series'], 'b-', 
                                    linewidth=2, label='Autocorrelation')
                    
                    # Trend line
                    if len(csd['autocorr_series']) > 10:
                        t = np.arange(len(csd['autocorr_series']))
                        trend = np.polyfit(t, csd['autocorr_series'], 1)
                        trend_line = np.polyval(trend, t)
                        self.csd_ax.plot(ac_time, trend_line, 'r--', 
                                        linewidth=2, label=f'Trend (slope={trend[0]:.4f})')
                    
                    self.csd_ax.set_title(f'Critical Slowing Down: {ind.title()}', 
                                         fontweight='bold')
                    self.csd_ax.set_xlabel('Time')
                    self.csd_ax.set_ylabel('Lag-1 Autocorrelation')
                    self.csd_ax.legend()
                    self.csd_ax.grid(True, alpha=0.3)
        
        # Update spectral analysis
        if self.indicators and self.indicators[0] in data_buffer:
            ind = self.indicators[0]
            if len(data_buffer[ind]) > 100:
                spectral = self.analyzers[ind].spectral_reddening(data_buffer[ind])
                
                if 'freqs' in spectral and 'power' in spectral:
                    self.spectral_ax.clear()
                    self.spectral_ax.loglog(spectral['freqs'], spectral['power'], 
                                           'b-', linewidth=1, alpha=0.7)
                    
                    self.spectral_ax.set_title(f'Power Spectrum: {ind.title()} ' +
                                              f'(β={spectral["power_law_exponent"]:.2f})',
                                              fontweight='bold')
                    self.spectral_ax.set_xlabel('Frequency')
                    self.spectral_ax.set_ylabel('Power')
                    self.spectral_ax.grid(True, alpha=0.3)
        
        # Update alert log
        self.alert_ax.clear()
        self.alert_ax.axis('off')
        
        # Display recent alerts
        alert_text = "RECENT ALERTS:\n" + "="*80 + "\n"
        
        all_alerts = []
        for analyzer in self.analyzers.values():
            all_alerts.extend(analyzer.alert_history[-5:])  # Last 5 alerts per indicator
        
        # Sort by timestamp
        all_alerts.sort(key=lambda x: x['timestamp'], reverse=True)
        
        for alert in all_alerts[:10]:  # Show last 10 alerts
            timestamp = alert['timestamp'].strftime('%H:%M:%S')
            score = alert['score']
            status = alert['status']
            alert_text += f"[{timestamp}] Score: {score:.2f} - {status}\n"
        
        if not all_alerts:
            alert_text += "No alerts. System stable.\n"
        
        self.alert_ax.text(0.05, 0.95, alert_text, transform=self.alert_ax.transAxes,
                          fontsize=9, verticalalignment='top', family='monospace',
                          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    def run(self, duration: int = None):
        """
        Run dashboard animation.
        
        Parameters:
        -----------
        duration : int
            Number of frames to run (None = infinite)
        """
        anim = FuncAnimation(self.fig, self.update, frames=duration,
                           interval=self.update_interval, repeat=False)
        plt.show()
        
        return anim


# ============================================================================
# SECTION 4: BATCH ANALYSIS TOOLS
# ============================================================================

class BatchAnalysisLab:
    """
    Laboratory for batch analysis of historical climate data.
    """
    
    def __init__(self):
        self.analyzer = EarlyWarningAnalyzer()
    
    def load_csv_data(self, filename: str, date_column: str = 'date',
                     value_column: str = 'value') -> pd.DataFrame:
        """
        Load climate data from CSV file.
        
        Parameters:
        -----------
        filename : str
            Path to CSV file
        date_column : str
            Name of date column
        value_column : str
            Name of value column
            
        Returns:
        --------
        data : pd.DataFrame
            Loaded data
        """
        df = pd.DataFrame(pd.read_csv(filename))
        if date_column in df.columns:
            df[date_column] = pd.to_datetime(df[date_column])
            df = df.sort_values(date_column)
        
        return df
    
    def analyze_historical_series(self, data: np.ndarray, 
                                 time: np.ndarray = None,
                                 indicator_name: str = "Climate Indicator") -> Dict:
        """
        Comprehensive analysis of historical time series.
        
        Parameters:
        -----------
        data : np.ndarray
            Historical time series data
        time : np.ndarray
            Time points (optional)
        indicator_name : str
            Name of indicator
            
        Returns:
        --------
        analysis : dict
            Comprehensive analysis results
        """
        if time is None:
            time = np.arange(len(data))
        
        print(f"\n{'='*70}")
        print(f"HISTORICAL ANALYSIS: {indicator_name}")
        print(f"{'='*70}")
        print(f"Data points: {len(data)}")
        print(f"Time span: {time[0]:.1f} to {time[-1]:.1f}")
        
        # Run all analyses
        composite = self.analyzer.composite_warning_score(data

)
# Statistical summary
    stats_summary = {
        'mean': np.mean(data),
        'std': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'trend': np.polyfit(time, data, 1)[0]
    }
    
    print(f"\nStatistical Summary:")
    print(f"  Mean: {stats_summary['mean']:.4f}")
    print(f"  Std Dev: {stats_summary['std']:.4f}")
    print(f"  Range: [{stats_summary['min']:.4f}, {stats_summary['max']:.4f}]")
    print(f"  Linear Trend: {stats_summary['trend']:.6f} per time unit")
    
    print(f"\nEarly Warning Analysis:")
    print(f"  Composite Score: {composite['composite_score']:.3f}")
    print(f"  Overall Status: {composite['overall_status']}")
    
    print(f"\n  Critical Slowing Down:")
    print(f"    Warning Level: {composite['csd']['warning_level']:.3f}")
    print(f"    Status: {composite['csd']['status']}")
    print(f"    Autocorrelation: {composite['csd']['autocorr']:.4f}")
    print(f"    Autocorr Trend: {composite['csd']['autocorr_trend']:.6f}")
    
    print(f"\n  Spectral Reddening:")
    print(f"    Warning Level: {composite['spectral']['warning_level']:.3f}")
    print(f"    Reddening Index: {composite['spectral']['reddening_index']:.4f}")
    print(f"    Power Law Exponent: {composite['spectral']['power_law_exponent']:.4f}")
    
    print(f"\n  Flickering:")
    print(f"    Warning Level: {composite['flickering']['warning_level']:.3f}")
    print(f"    Flickering Rate: {composite['flickering']['flickering_rate']:.4f}")
    print(f"    State Balance: {composite['flickering']['state_balance']:.4f}")
    
    print(f"\n{'='*70}")
    
    return {
        'indicator_name': indicator_name,
        'statistics': stats_summary,
        'early_warning': composite,
        'time': time,
        'data': data
    }

def compare_multiple_indicators(self, datasets: Dict[str, np.ndarray]) -> Dict:
    """
    Compare early warning signals across multiple indicators.
    
    Parameters:
    -----------
    datasets : dict
        Dictionary of {indicator_name: data_array}
        
    Returns:
    --------
    comparison : dict
        Comparative analysis
    """
    print(f"\n{'='*70}")
    print(f"MULTI-INDICATOR COMPARISON")
    print(f"{'='*70}")
    
    results = {}
    warning_scores = {}
    
    for name, data in datasets.items():
        composite = self.analyzer.composite_warning_score(data)
        results[name] = composite
        warning_scores[name] = composite['composite_score']
    
    # Rank by warning level
    ranked = sorted(warning_scores.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\nRanking by Warning Level:")
    for i, (name, score) in enumerate(ranked):
        status = results[name]['overall_status']
        print(f"  {i+1}. {name:20s}: {score:.3f} - {status}")
    
    # System-wide risk
    avg_warning = np.mean(list(warning_scores.values()))
    max_warning = np.max(list(warning_scores.values()))
    
    print(f"\nSystem-Wide Assessment:")
    print(f"  Average Warning Level: {avg_warning:.3f}")
    print(f"  Maximum Warning Level: {max_warning:.3f}")
    print(f"  Indicators at HIGH risk: {sum(1 for s in warning_scores.values() if s > 0.5)}")
    print(f"  Indicators at CRITICAL risk: {sum(1 for s in warning_scores.values() if s > 0.7)}")
    
    print(f"\n{'='*70}")
    
    return {
        'results': results,
        'ranking': ranked,
        'system_average': avg_warning,
        'system_maximum': max_warning
    }

def generate_report(self, analysis: Dict, filename: str = None) -> str:
    """
    Generate detailed analysis report.
    
    Parameters:
    -----------
    analysis : dict
        Analysis results from analyze_historical_series
    filename : str
        Optional file to save report
        
    Returns:
    --------
    report : str
        Formatted report text
    """
    report = []
    report.append("="*80)
    report.append("CLIMATE TIPPING POINT EARLY WARNING ANALYSIS REPORT")
    report.append("="*80)
    report.append(f"\nIndicator: {analysis['indicator_name']}")
    report.append(f"Analysis Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"Data Points: {len(analysis['data'])}")
    
    report.append("\n" + "-"*80)
    report.append("STATISTICAL SUMMARY")
    report.append("-"*80)
    stats = analysis['statistics']
    report.append(f"Mean Value: {stats['mean']:.4f}")
    report.append(f"Standard Deviation: {stats['std']:.4f}")
    report.append(f"Range: [{stats['min']:.4f}, {stats['max']:.4f}]")
    report.append(f"Linear Trend: {stats['trend']:.6f} per time unit")
    
    report.append("\n" + "-"*80)
    report.append("EARLY WARNING SIGNAL ANALYSIS")
    report.append("-"*80)
    
    ew = analysis['early_warning']
    report.append(f"\nCOMPOSITE WARNING SCORE: {ew['composite_score']:.3f}")
    report.append(f"OVERALL STATUS: {ew['overall_status']}")
    
    report.append(f"\n1. CRITICAL SLOWING DOWN (CSD)")
    report.append(f"   Warning Level: {ew['csd']['warning_level']:.3f}")
    report.append(f"   Status: {ew['csd']['status']}")
    report.append(f"   Current Autocorrelation: {ew['csd']['autocorr']:.4f}")
    report.append(f"   Autocorrelation Trend: {ew['csd']['autocorr_trend']:.6f}")
    report.append(f"   Current Variance: {ew['csd']['variance']:.4f}")
    report.append(f"   Variance Trend: {ew['csd']['variance_trend']:.6f}")
    
    report.append(f"\n2. SPECTRAL REDDENING")
    report.append(f"   Warning Level: {ew['spectral']['warning_level']:.3f}")
    report.append(f"   Reddening Index: {ew['spectral']['reddening_index']:.4f}")
    report.append(f"   Power Law Exponent: {ew['spectral']['power_law_exponent']:.4f}")
    report.append(f"   Low Freq Power: {ew['spectral']['low_freq_power']:.4e}")
    report.append(f"   High Freq Power: {ew['spectral']['high_freq_power']:.4e}")
    
    report.append(f"\n3. FLICKERING")
    report.append(f"   Warning Level: {ew['flickering']['warning_level']:.3f}")
    report.append(f"   Flickering Rate: {ew['flickering']['flickering_rate']:.4f}")
    report.append(f"   Number of Transitions: {ew['flickering']['transitions']}")
    report.append(f"   State Balance: {ew['flickering']['state_balance']:.4f}")
    
    report.append("\n" + "-"*80)
    report.append("INTERPRETATION")
    report.append("-"*80)
    
    if ew['composite_score'] > 0.7:
        report.append("\nCRITICAL WARNING: System shows strong signs of approaching tipping point.")
        report.append("RECOMMENDED ACTIONS:")
        report.append("  - Immediate enhanced monitoring")
        report.append("  - Alert decision-makers")
        report.append("  - Prepare emergency response protocols")
        report.append("  - Implement intervention measures if available")
    elif ew['composite_score'] > 0.5:
        report.append("\nHIGH RISK: System showing elevated warning signals.")
        report.append("RECOMMENDED ACTIONS:")
        report.append("  - Increase monitoring frequency")
        report.append("  - Notify relevant stakeholders")
        report.append("  - Review intervention options")
        report.append("  - Conduct additional analysis")
    elif ew['composite_score'] > 0.3:
        report.append("\nMODERATE RISK: Some warning signals detected.")
        report.append("RECOMMENDED ACTIONS:")
        report.append("  - Continue regular monitoring")
        report.append("  - Watch for trend development")
        report.append("  - Prepare contingency plans")
    else:
        report.append("\nLOW RISK: System appears stable.")
        report.append("RECOMMENDED ACTIONS:")
        report.append("  - Maintain routine monitoring")
        report.append("  - Continue data collection")
    
    report.append("\n" + "="*80)
    report.append("END OF REPORT")
    report.append("="*80)
    
    report_text = "\n".join(report)
    
    if filename:
        with open(filename, 'w') as f:
            f.write(report_text)
        print(f"\nReport saved to: {filename}")
    
    return report_text
============================================================================
SECTION 5: MACHINE LEARNING PREDICTOR
============================================================================
class MLTippingPredictor:
"""
Machine learning-based tipping point prediction.
"""
def __init__(self):
    self.scaler = StandardScaler()
    self.pca = PCA(n_components=5)
    self.is_fitted = False

def extract_features(self, data: np.ndarray, window: int = 100) -> np.ndarray:
    """
    Extract features for ML prediction.
    
    Parameters:
    -----------
    data : np.ndarray
        Time series data
    window : int
        Feature extraction window
        
    Returns:
    --------
    features : np.ndarray
        Feature vector
    """
    if len(data) < window:
        return np.zeros(15)  # Return empty features
    
    # Use most recent window
    segment = data[-window:]
    
    # Statistical features
    mean = np.mean(segment)
    std = np.std(segment)
    skew = stats.skew(segment)
    kurt = stats.kurtosis(segment)
    
    # Trend features
    x = np.arange(len(segment))
    trend_coef = np.polyfit(x, segment, 1)[0]
    
    # Autocorrelation features
    ac1 = np.corrcoef(segment[:-1], segment[1:])[0, 1] if len(segment) > 1 else 0
    ac5 = np.corrcoef(segment[:-5], segment[5:])[0, 1] if len(segment) > 5 else 0
    
    # Detrend for variance analysis
    detrended = segment - np.polyval(np.polyfit(x, segment, 1), x)
    variance = np.var(detrended)
    
    # Spectral features
    if len(segment) > 10:
        freqs = fftfreq(len(segment))
        power = np.abs(fft(detrended))**2
        pos_mask = freqs > 0
        
        if np.sum(pos_mask) > 0:
            median_freq = np.median(freqs[pos_mask])
            low_power = np.mean(power[pos_mask & (freqs < median_freq)])
            high_power = np.mean(power[pos_mask & (freqs >= median_freq)])
            spectral_ratio = low_power / (high_power + 1e-10)
        else:
            spectral_ratio = 0
    else:
        spectral_ratio = 0
    
    # Return rate features
    abs_segment = np.abs(detrended)
    threshold = np.std(abs_segment)
    return_times = []
    
    for i in range(len(abs_segment)):
        if abs_segment[i] > threshold:
            # Find return time
            for j in range(i+1, min(i+20, len(abs_segment))):
                if abs_segment[j] < threshold/2:
                    return_times.append(j - i)
                    break
    
    avg_return_time = np.mean(return_times) if return_times else 0
    
    # Flickering features
    median = np.median(segment)
    crossings = np.sum(np.abs(np.diff((segment > median).astype(int))))
    
    # Range features
    range_val = np.max(segment) - np.min(segment)
    
    # Combine all features
    features = np.array([
        mean, std, skew, kurt, trend_coef,
        ac1, ac5, variance, spectral_ratio,
        avg_return_time, crossings, range_val,
        np.percentile(segment, 25),
        np.percentile(segment, 75),
        len(segment)
    ])
    
    return features

def predict_time_to_tipping(self, data: np.ndarray) -> Dict:
    """
    Predict time until potential tipping point.
    
    Parameters:
    -----------
    data : np.ndarray
        Time series data
        
    Returns:
    --------
    prediction : dict
        Prediction results
    """
    # Extract features
    features = self.extract_features(data)
    
    # Simple heuristic model (in production, would use trained ML model)
    # Based on early warning signal strength
    
    analyzer = EarlyWarningAnalyzer()
    composite = analyzer.composite_warning_score(data)
    
    warning_level = composite['composite_score']
    
    # Estimate time to tipping based on current trend and warning level
    if len(data) > 100:
        trend = np.polyfit(np.arange(len(data)), data, 1)[0]
        
        # If approaching threshold and warning signals high
        if warning_level > 0.7:
            time_to_tip = "IMMINENT (0-10 time units)"
            confidence = 0.8
        elif warning_level > 0.5:
            # Estimate based on trend
            if abs(trend) > 0:
                estimated = int(50 / (warning_level * abs(trend)))
                time_to_tip = f"NEAR-TERM ({estimated}-{estimated*2} time units)"
            else:
                time_to_tip = "UNCERTAIN (trend unclear)"
            confidence = 0.6
        elif warning_level > 0.3:
            time_to_tip = "MEDIUM-TERM (>50 time units)"
            confidence = 0.4
        else:
            time_to_tip = "NOT DETECTED (system stable)"
            confidence = 0.7
    else:
        time_to_tip = "INSUFFICIENT DATA"
        confidence = 0.0
    
    return {
        'time_to_tipping': time_to_tip,
        'confidence': confidence,
        'warning_level': warning_level,
        'features': features,
        'recommendation': self._get_recommendation(warning_level)
    }

def _get_recommendation(self, warning_level: float) -> str:
    """Generate recommendation based on warning level."""
    if warning_level > 0.7:
        return "URGENT: Implement immediate intervention measures"
    elif warning_level > 0.5:
        return "HIGH PRIORITY: Prepare intervention, enhance monitoring"
    elif warning_level > 0.3:
        return "MODERATE: Increase monitoring frequency, review protocols"
    else:
        return "ROUTINE: Continue standard monitoring"
============================================================================
SECTION 6: VISUALIZATION SUITE
============================================================================
class VisualizationSuite:
"""
Comprehensive visualization tools for early warning analysis.
"""
@staticmethod
def plot_comprehensive_analysis(analysis: Dict):
    """
    Create comprehensive multi-panel visualization.
    
    Parameters:
    -----------
    analysis : dict
        Analysis results from BatchAnalysisLab
    """
    fig = plt.figure(figsize=(16, 12))
    gs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.35)
    
    data = analysis['data']
    time = analysis['time']
    ew = analysis['early_warning']
    
    # 1. Time series with trend
    ax1 = fig.add_subplot(gs[0, :])
    ax1.plot(time, data, 'b-', linewidth=1, alpha=0.7, label='Data')
    
    # Trend
    trend = np.polyfit(time, data, 1)
    trend_line = np.polyval(trend, time)
    ax1.plot(time, trend_line, 'r--', linewidth=2, label=f'Trend (slope={trend[0]:.4f})')
    
    ax1.set_xlabel('Time', fontsize=11)
    ax1.set_ylabel('Value', fontsize=11)
    ax1.set_title(f'{analysis["indicator_name"]} - Time Series', 
                 fontsize=13, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Autocorrelation evolution
    ax2 = fig.add_subplot(gs[1, 0])
    if 'autocorr_series' in ew['csd'] and ew['csd']['autocorr_series']:
        ac_series = ew['csd']['autocorr_series']
        ac_time = time[-len(ac_series):]
        ax2.plot(ac_time, ac_series, 'b-', linewidth=2)
        
        # Trend
        t = np.arange(len(ac_series))
        ac_trend = np.polyfit(t, ac_series, 1)
        ax2.plot(ac_time, np.polyval(ac_trend, t), 'r--', linewidth=2)
    
    ax2.set_xlabel('Time', fontsize=10)
    ax2.set_ylabel('Autocorrelation', fontsize=10)
    ax2.set_title('Critical Slowing Down', fontsize=11, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # 3. Variance evolution
    ax3 = fig.add_subplot(gs[1, 1])
    if 'variance_series' in ew['csd'] and ew['csd']['variance_series']:
        var_series = ew['csd']['variance_series']
        var_time = time[-len(var_series):]
        ax3.plot(var_time, var_series, 'g-', linewidth=2)
        
        # Trend
        t = np.arange(len(var_series))
        var_trend = np.polyfit(t, var_series, 1)
        ax3.plot(var_time, np.polyval(var_trend, t), 'r--', linewidth=2)
    
    ax3.set_xlabel('Time', fontsize=10)
    ax3.set_ylabel('Variance', fontsize=10)
    ax3.set_title('Variance Evolution', fontsize=11, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    
    # 4. Power spectrum
    ax4 = fig.add_subplot(gs[1, 2])
    if 'freqs' in ew['spectral'] and 'power' in ew['spectral']:
        freqs = ew['spectral']['freqs']
        power = ew['spectral']['power']
        ax4.loglog(freqs, power, 'b-', linewidth=1, alpha=0.7)
        
        # Power law fit
        beta = ew['spectral']['power_law_exponent']
        ax4.text(0.05, 0.95, f'β = {beta:.2f}', 
                transform=ax4.transAxes, fontsize=10,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    ax4.set_xlabel('Frequency', fontsize=10)
    ax4.set_ylabel('Power', fontsize=10)
    ax4.set_title('Power Spectrum', fontsize=11, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # 5. Warning level gauge
    ax5 = fig.add_subplot(gs[2, 0])
    score = ew['composite_score']
    color = ew['status_color']
    
    # Create gauge
    theta = np.linspace(0, np.pi, 100)
    r = np.ones_like(theta)
    
    ax5 = plt.subplot(gs[2, 0], projection='polar')
    ax5.plot(theta, r, 'k-', linewidth=2)
    ax5.fill_between(theta, 0, r, alpha=0.1, color='gray')
    
    # Color zones
    ax5.fill_between(theta[theta < np.pi*0.3], 0, 1, alpha=0.3, color='green')
    ax5.fill_between(theta[(theta >= np.pi*0.3) & (theta < np.pi*0.5)], 0, 1, 
                    alpha=0.3, color='yellow')
    ax5.fill_between(theta[(theta >= np.pi*0.5) & (theta < np.pi*0.7)], 0, 1,
                    alpha=0.3, color='orange')
    ax5.fill_between(theta[theta >= np.pi*0.7], 0, 1, alpha=0.3, color='red')
    
    # Needle
    needle_angle = score * np.pi
    ax5.plot([needle_angle, needle_angle], [0, 1], 'r-', linewidth=3)
    ax5.plot(needle_angle, 1, 'ro', markersize=10)
    
    ax5.set_ylim(0, 1.2)
    ax5.set_theta_offset(np.pi)
    ax5.set_theta_direction(-1)
    ax5.set_xticks([0, np.pi*0.3, np.pi*0.5, np.pi*0.7, np.pi])
    ax5.set_xticklabels(['1.0', '0.7', '0.5', '0.3', '0.0'])
    ax5.set_yticks([])
    ax5.set_title('Warning Level', fontsize=11, fontweight='bold', pad=20)
    
    # 6. Signal components
    ax6 = fig.add_subplot(gs[2, 1])
    components = ['CSD', 'Spectral', 'Flickering']
    values = [
        ew['csd']['warning_level'],
        ew['spectral']['warning_level'],
        ew['flickering']['warning_level']
    ]
    colors_comp = [ew['status_color']] * 3
    
    ax6.barh(components, values, color=colors_comp, alpha=0.7)
    ax6.set_xlim(0, 1)
    ax6.set_xlabel('Warning Level', fontsize=10)
    ax6.set_title('Signal Components', fontsize=11, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='x')
    
    # 7. Status summary
    ax7 = fig.add_subplot(gs[2, 2])
    ax7.axis('off')
    
    summary_text = f"ANALYSIS SUMMARY\n{'='*30}\n\n"
    summary_text += f"Composite Score: {score:.3f}\n"
    summary_text += f"Status: {ew['overall_status']}\n\n"
    summary_text += f"Critical Slowing Down:\n"
    summary_text += f"  Level: {ew['csd']['warning_level']:.3f}\n"
    summary_text += f"  Status: {ew['csd']['status']}\n\n"
    summary_text += f"Spectral Reddening:\n"
    summary_text += f"  Index: {ew['spectral']['reddening_index']:.2f}\n\n"
    summary_text += f"Flickering:\n"
    summary_text += f"  Rate: {ew['flickering']['flickering_rate']:.3f}\n"
    
    ax7.text(0.1, 0.9, summary_text, transform=ax7.transAxes,
            fontsize=9, verticalalignment='top', family='monospace',
            bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))
    
    fig.suptitle(f'Comprehensive Early Warning Analysis: {analysis["indicator_name"]}',
                fontsize=15, fontweight='bold')
    
    plt.tight_layout()
    return fig

@staticmethod
def plot_multi_indicator_comparison(comparison: Dict):
    """
    Visualize comparison across multiple indicators.
    
    Parameters:
    -----------
    comparison : dict
        Results from compare_multiple_indicators
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Ranking bar chart
    names = [item[0] for item in comparison['ranking']]
    scores = [item[1] for item in comparison['ranking']]
    colors = [comparison['results'][name]['status_color'] for name in names]
    
    y_pos = np.arange(len(names))
    ax1.barh(y_pos, scores, color=colors, alpha=0.7)
    ax1.set_yticks(y_pos)
    ax1.set_yticklabels([n.replace('_', '\n') for n in names])
    ax1.set_xlabel('Composite Warning Score', fontsize=12)
    ax1.set_title('Indicator Warning Level Ranking', fontsize=13, fontweight='bold')
    ax1.axvline(0.5, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='High Risk')
    ax1.axvline(0.7, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Critical Risk')
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='x')
    
    # Component breakdown radar chart
    categories = ['CSD', 'Spectral', 'Flickering']
    n_cats = len(categories)
    
    angles = np.linspace(0, 2*np.pi, n_cats, endpoint=False).tolist()
    angles += angles[:1]
    
    ax2 = plt.subplot(122, projection='polar')
    
    for name in names[:5]:  # Top 5 indicators
        values = [
            comparison['results'][name]['csd']['warning_level'],
            comparison['results'][name]['spectral']['warning_level'],
            comparison['results'][name]['flickering']['warning_level']
        ]
        values += values[:1]
        
        ax2.plot(angles, values, 'o-', linewidth=2, label=name[:15])
        ax2.fill(angles, values, alpha=0.1)
    
    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(categories)
    ax2.set_ylim(0, 1)
    ax2.set_title('Warning Signal Components\n(Top 5 Indicators)', 
                 fontsize=13, fontweight='bold', pad=20)
    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)
    ax2.grid(True)
    
    plt.tight_layout()
    return fig
============================================================================
SECTION 7: DEMONSTRATIONS AND EXAMPLES
============================================================================
def demo_realtime_dashboard():
"""Demonstrate real-time monitoring dashboard."""
print("\nStarting Real-Time Early Warning Dashboard...")
print("This will run for 100 time steps. Close the window to stop.")
indicators = ['temperature', 'arctic_ice', 'amoc', 'amazon', 'permafrost', 'glaciers']
dashboard = EarlyWarningDashboard(indicators, update_interval=200)
dashboard.run(duration=100)
def demo_historical_analysis():
"""Demonstrate historical data analysis."""
print("\nHistorical Analysis Demonstration")
print("="*70)
# Generate synthetic historical data
time = np.linspace(0, 100, 1000)

# Simulate approaching tipping point
baseline = 1.0
trend = 0.005  # Gradual warming

# Add increasing autocorrelation (critical slowing down)
data = np.zeros_like(time)
data[0] = baseline

for i in range(1, len(time)):
    # Autocorrelation increases over time
    autocorr = 0.3 + 0.6 * (i / len(time))
    noise = np.random.randn() * 0.1
    data[i] = autocorr * data[i-1] + (1-autocorr) * (baseline + trend * time[i]) + noise

# Analyze
lab = BatchAnalysisLab()
analysis = lab.analyze_historical_series(data, time, "Global Temperature Anomaly")

# Visualize
viz = VisualizationSuite()
fig = viz.plot_comprehensive_analysis(analysis)
plt.show()

# Generate report
report = lab.generate_report(analysis, "temperature_analysis_report.txt")

return analysis
def demo_multi_indicator():
"""Demonstrate multi-indicator comparison."""
print("\nMulti-Indicator Comparison Demonstration")
print("="*70)
# Generate synthetic data for multiple indicators
n_points = 500

datasets = {}

# Temperature - high warning
t_data = np.cumsum(np.random.randn(n_points) * 0.05) + 1.2
datasets['temperature'] = t_data

# Arctic ice - critical warning
ice_data = 6.5 - np.linspace(0, 2.5, n_points) + np.cumsum(np.random.randn(n_points) * 0.05)
datasets['arctic_ice'] = ice_data

# AMOC - moderate warning
amoc_data = 100 - np.linspace(0, 10, n_points) + np.random.randn(n_points) * 2
datasets['amoc'] = amoc_data

# Amazon - stable
amazon_data = 85 + np
.random.randn(n_points) * 1
datasets['amazon'] = amazon_data
# Permafrost - high warning
perm_data = np.linspace(5, 20, n_points) + np.cumsum(np.random.randn(n_points) * 0.3)
datasets['permafrost'] = perm_data

# Glaciers - moderate warning
glacier_data = 85 - np.linspace(0, 15, n_points) + np.random.randn(n_points) * 1.5
datasets['glaciers'] = glacier_data

# Analyze and compare
lab = BatchAnalysisLab()
comparison = lab.compare_multiple_indicators(datasets)

# Visualize
viz = VisualizationSuite()
fig = viz.plot_multi_indicator_comparison(comparison)
plt.show()

return comparison
def demo_ml_prediction():
"""Demonstrate ML-based tipping point prediction."""
print("\nML Tipping Point Prediction Demonstration")
print("="*70)
# Generate data approaching tipping point
time = np.linspace(0, 100, 500)

# Simulate accelerating approach to threshold
data = 1.0 + 0.5 * (1 - np.exp(-time/30))

# Add increasing variance (critical slowing down)
for i in range(len(data)):
    variance = 0.05 * (1 + i/len(data))
    data[i] += np.random.randn() * variance

# Predict
predictor = MLTippingPredictor()
prediction = predictor.predict_time_to_tipping(data)

print(f"\nPrediction Results:")
print(f"  Time to Tipping: {prediction['time_to_tipping']}")
print(f"  Confidence: {prediction['confidence']*100:.1f}%")
print(f"  Warning Level: {prediction['warning_level']:.3f}")
print(f"  Recommendation: {prediction['recommendation']}")

# Visualize
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Time series
ax1.plot(time, data, 'b-', linewidth=1.5, label='Data')
ax1.axhline(1.5, color='red', linestyle='--', linewidth=2, label='Threshold')
ax1.set_xlabel('Time', fontsize=11)
ax1.set_ylabel('Value', fontsize=11)
ax1.set_title('Time Series with Tipping Point Prediction', fontsize=13, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Add prediction annotation
ax1.text(0.98, 0.95, f"Prediction: {prediction['time_to_tipping']}\nConfidence: {prediction['confidence']*100:.0f}%",
        transform=ax1.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right',
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# Features
features = prediction['features']
feature_names = ['Mean', 'Std', 'Skew', 'Kurt', 'Trend', 'AC1', 'AC5', 'Var',
                'Spectral', 'Return', 'Cross', 'Range', 'Q25', 'Q75', 'N']

ax2.bar(range(len(features)), features, alpha=0.7)
ax2.set_xticks(range(len(features)))
ax2.set_xticklabels(feature_names, rotation=45, ha='right')
ax2.set_ylabel('Feature Value', fontsize=11)
ax2.set_title('Extracted Features for ML Prediction', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

return prediction
def demo_satellite_data_integration():
"""
Demonstrate integration with satellite/remote sensing data.
(Simulated - in production would connect to actual data sources)
"""
print("\nSatellite Data Integration Demonstration")
print("="*70)
print("\nSimulating data from:")
print("  - NASA MODIS (Arctic sea ice)")
print("  - ESA Copernicus Sentinel (Land cover)")
print("  - NOAA AVHRR (Sea surface temperature)")
# Simulate satellite observation timestamps
dates = pd.date_range('2020-01-01', '2026-01-01', freq='M')

# Simulate Arctic sea ice extent from satellite
baseline_ice = 6.5
ice_extent = baseline_ice - np.linspace(0, 2.0, len(dates))
ice_extent += np.random.randn(len(dates)) * 0.2

# Create DataFrame
df = pd.DataFrame({
    'date': dates,
    'ice_extent': ice_extent,
    'source': 'NASA_MODIS'
})

print(f"\nData Summary:")
print(f"  Time Period: {dates[0].date()} to {dates[-1].date()}")
print(f"  Observations: {len(dates)}")
print(f"  Current Ice Extent: {ice_extent[-1]:.2f} million km²")
print(f"  Trend: {np.polyfit(range(len(ice_extent)), ice_extent, 1)[0]:.4f} million km²/month")

# Analyze
lab = BatchAnalysisLab()
analysis = lab.analyze_historical_series(
    ice_extent,
    np.arange(len(ice_extent)),
    "Arctic Sea Ice Extent (Satellite)"
)

# Early warning detection
if analysis['early_warning']['composite_score'] > 0.5:
    print(f"\n⚠️  ALERT: High early warning signal detected!")
    print(f"   Warning Level: {analysis['early_warning']['composite_score']:.3f}")
    print(f"   Status: {analysis['early_warning']['overall_status']}")

# Visualize
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Time series
axes[0, 0].plot(dates, ice_extent, 'b-', linewidth=2, marker='o', markersize=4)
axes[0, 0].axhline(3.0, color='red', linestyle='--', linewidth=2, label='Critical Threshold')
axes[0, 0].set_xlabel('Date', fontsize=11)
axes[0, 0].set_ylabel('Ice Extent (million km²)', fontsize=11)
axes[0, 0].set_title('Arctic Sea Ice Extent - Satellite Observations', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].tick_params(axis='x', rotation=45)

# Warning gauge
score = analysis['early_warning']['composite_score']
theta = np.linspace(0, np.pi, 100)

ax_polar = plt.subplot(2, 2, 2, projection='polar')
ax_polar.plot(theta, np.ones_like(theta), 'k-', linewidth=2)
ax_polar.fill_between(theta[theta < np.pi*0.3], 0, 1, alpha=0.3, color='green')
ax_polar.fill_between(theta[(theta >= np.pi*0.3) & (theta < np.pi*0.7)], 0, 1, alpha=0.3, color='yellow')
ax_polar.fill_between(theta[theta >= np.pi*0.7], 0, 1, alpha=0.3, color='red')

needle_angle = score * np.pi
ax_polar.plot([needle_angle, needle_angle], [0, 1], 'r-', linewidth=4)
ax_polar.plot(needle_angle, 1, 'ro', markersize=12)

ax_polar.set_ylim(0, 1.2)
ax_polar.set_theta_offset(np.pi)
ax_polar.set_theta_direction(-1)
ax_polar.set_yticks([])
ax_polar.set_title(f'Early Warning Level\n{score:.3f}', fontsize=12, fontweight='bold', pad=20)

# Autocorrelation
if 'autocorr_series' in analysis['early_warning']['csd']:
    ac = analysis['early_warning']['csd']['autocorr_series']
    axes[1, 0].plot(ac, 'b-', linewidth=2)
    axes[1, 0].set_xlabel('Time Window', fontsize=11)
    axes[1, 0].set_ylabel('Autocorrelation', fontsize=11)
    axes[1, 0].set_title('Critical Slowing Down Signal', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)

# Spectral analysis
if 'freqs' in analysis['early_warning']['spectral']:
    freqs = analysis['early_warning']['spectral']['freqs']
    power = analysis['early_warning']['spectral']['power']
    axes[1, 1].loglog(freqs, power, 'b-', linewidth=1.5)
    axes[1, 1].set_xlabel('Frequency', fontsize=11)
    axes[1, 1].set_ylabel('Power', fontsize=11)
    axes[1, 1].set_title('Power Spectrum', fontsize=12, fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

return df, analysis
def create_custom_monitoring_system():
"""
Create custom monitoring system for specific use case.
"""
print("\nCustom Monitoring System Setup")
print("="*70)
print("\nThis example shows how to set up a custom monitoring system")
print("for a specific climate indicator.\n")

# Example: Monitoring Greenland Ice Sheet mass balance
print("Example: Greenland Ice Sheet Mass Balance Monitor")
print("-" * 70)

# Configuration
config = {
    'indicator_name': 'Greenland Ice Sheet Mass Balance',
    'units': 'Gt/year',
    'critical_threshold': -300,  # Gigatons per year
    'warning_threshold': -200,
    'update_frequency': 'monthly',
    'data_sources': ['NASA GRACE', 'ESA CryoSat-2'],
    'alert_contacts': ['climate-team@example.org']
}

print("\nConfiguration:")
for key, value in config.items():
    print(f"  {key}: {value}")

# Initialize monitoring
analyzer = EarlyWarningAnalyzer(window_size=36)  # 3-year window for monthly data

# Simulate incoming data stream
print("\nSimulating data stream...")
time_points = np.arange(120)  # 10 years of monthly data

# Simulate accelerating mass loss
mass_balance = -150 - 5*time_points - 0.05*time_points**2
mass_balance += np.random.randn(len(time_points)) * 20

# Monitor with alerts
alerts = []
for i, (t, value) in enumerate(zip(time_points, mass_balance)):
    if i > 36:  # Need sufficient data
        analysis = analyzer.composite_warning_score(mass_balance[:i+1])
        
        # Check for alerts
        if analysis['composite_score'] > 0.7 and i % 12 == 0:  # Annual alert check
            alert = {
                'time': t,
                'value': value,
                'warning_score': analysis['composite_score'],
                'status': analysis['overall_status'],
                'message': f"CRITICAL: Early warning score {analysis['composite_score']:.3f}"
            }
            alerts.append(alert)
            print(f"\n🚨 ALERT at time {t}: {alert['message']}")

print(f"\n\nTotal alerts generated: {len(alerts)}")

# Summary visualization
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

ax1.plot(time_points, mass_balance, 'b-', linewidth=2, label='Mass Balance')
ax1.axhline(config['warning_threshold'], color='orange', linestyle='--', 
           linewidth=2, label='Warning Threshold')
ax1.axhline(config['critical_threshold'], color='red', linestyle='--',
           linewidth=2, label='Critical Threshold')

# Mark alerts
for alert in alerts:
    ax1.axvline(alert['time'], color='red', alpha=0.3, linestyle=':')

ax1.set_xlabel('Time (months)', fontsize=11)
ax1.set_ylabel('Mass Balance (Gt/year)', fontsize=11)
ax1.set_title('Greenland Ice Sheet Mass Balance Monitoring', fontsize=13, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Warning level over time
warning_levels = []
for i in range(37, len(mass_balance)):
    analysis = analyzer.composite_warning_score(mass_balance[:i+1])
    warning_levels.append(analysis['composite_score'])

ax2.plot(time_points[37:], warning_levels, 'r-', linewidth=2)
ax2.axhline(0.5, color='orange', linestyle='--', alpha=0.5, label='High Risk')
ax2.axhline(0.7, color='red', linestyle='--', alpha=0.5, label='Critical Risk')
ax2.fill_between(time_points[37:], 0, warning_levels, alpha=0.3, color='red')

ax2.set_xlabel('Time (months)', fontsize=11)
ax2.set_ylabel('Early Warning Score', fontsize=11)
ax2.set_title('Early Warning Signal Evolution', fontsize=13, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0, 1)

plt.tight_layout()
plt.show()

return config, alerts
============================================================================
SECTION 8: MAIN INTERFACE
============================================================================
def main():
"""Main entry point with interactive menu."""
print("\n" + "="*70)
print("CLIMATE TIPPING POINT EARLY WARNING DATA LAB")
print("Real-time Monitoring and Analysis System")
print("="*70)
print("\nAvailable Demonstrations:")
print("1. Real-time Monitoring Dashboard")
print("2. Historical Data Analysis")
print("3. Multi-Indicator Comparison")
print("4. ML Tipping Point Prediction")
print("5. Satellite Data Integration")
print("6. Custom Monitoring System Setup")
print("7. Run All Demonstrations")
print("0. Exit")

choice = input("\nSelect demonstration (0-7): ").strip()

if choice == '1':
    demo_realtime_dashboard()
elif choice == '2':
    demo_historical_analysis()
elif choice == '3':
    demo_multi_indicator()
elif choice == '4':
    demo_ml_prediction()
elif choice == '5':
    demo_satellite_data_integration()
elif choice == '6':
    create_custom_monitoring_system()
elif choice == '7':
    print("\nRunning all demonstrations...")
    print("\n1/6: Real-time Dashboard (will open in new window)")
    input("Press Enter when ready...")
    # demo_realtime_dashboard()  # Comment out for batch run
    
    print("\n2/6: Historical Analysis")
    demo_historical_analysis()
    
    print("\n3/6: Multi-Indicator Comparison")
    demo_multi_indicator()
    
    print("\n4/6: ML Prediction")
    demo_ml_prediction()
    
    print("\n5/6: Satellite Data Integration")
    demo_satellite_data_integration()
    
    print("\n6/6: Custom Monitoring System")
    create_custom_monitoring_system()
    
    print("\n✓ All demonstrations complete!")
elif choice == '0':
    print("\nExiting...")
    return
else:
    print("\nInvalid choice")
if name == "main":
main()
---

## USAGE EXAMPLES

### Example 1: Quick Analysis of Your Data
```python
from climate_early_warning_lab import BatchAnalysisLab
import numpy as np

# Load your data
data = np.loadtxt('your_climate_data.txt')
time = np.arange(len(data))

# Analyze
lab = BatchAnalysisLab()
analysis = lab.analyze_historical_series(data, time, "Your Indicator")

# Get warning level
warning = analysis['early_warning']['composite_score']
print(f"Warning Level: {warning:.3f}")

# Generate report
report = lab.generate_report(analysis, "analysis_report.txt")
Example 2: Real-time Monitoring
from climate_early_warning_lab import EarlyWarningDashboard

# Monitor multiple indicators
indicators = ['temperature', 'arctic_ice', 'permafrost']
dashboard = EarlyWarningDashboard(indicators, update_interval=100)
dashboard.run()
Example 3: Custom Alert System
from climate_early_warning_lab import EarlyWarningAnalyzer
import numpy as np

analyzer = EarlyWarningAnalyzer()

# Your data stream
for new_data in your_data_stream:
    # Analyze current state
    analysis = analyzer.composite_warning_score(new_data)
    
    # Check for alerts
    if analysis['composite_score'] > 0.7:
        send_alert(f"CRITICAL WARNING: {analysis['overall_status']}")
    elif analysis['composite_score'] > 0.5:
        send_notification(f"High Risk: {analysis['overall_status']}")
COMMAND LINE USAGE
# Run interactive lab
python climate_early_warning_lab.py

# Or import and use programmatically
python -c "from climate_early_warning_lab import demo_historical_analysis; demo_historical_analysis()"
KEY FEATURES
✅ Real-time Monitoring Dashboard - Live visualization of multiple indicators
✅ Critical Slowing Down Detection - Autocorrelation and variance analysis
✅ Spectral Reddening - Power spectrum analysis for early warnings
✅ Flickering Detection - Rapid state transitions near tipping points
✅ ML Prediction - Time-to-tipping estimation
✅ Batch Historical Analysis - Comprehensive retrospective analysis
✅ Multi-indicator Comparison - System-wide risk assessment
✅ Automated Alerting - Configurable warning thresholds
✅ Comprehensive Reporting - Stakeholder-ready analysis documents
✅ Satellite Data Integration - Ready for NASA/Copernicus data streams
FOR NASA AWG & COPERNICUS
This lab provides:
Real-time Earth Observation Processing
Early Warning Signal Detection
Automated Alert Generation
Multi-sensor Data Fusion
Decision Support Tools
Scientific Reporting
Save as climate_early_warning_lab.py for your research! 🌍📊🛰️
